"""
Script de Benchmark e Performance para Mensagerias CMShop
========================================================

Este script executa testes de performance automatizados com diferentes
configuraÃ§Ãµes para medir o throughput e comportamento das filas sob carga.

CenÃ¡rios testados:
- Teste de throughput mÃ¡ximo
- Teste de concorrÃªncia
- Teste de stress (muitas mensagens)
- Teste de latÃªncia
- Benchmark comparativo
"""

import time
import statistics
import json
from datetime import datetime
from Test.test_messaging import CMShopMessageTester, Queues
import logging

# ConfiguraÃ§Ã£o de logging mais limpa para benchmark
logging.basicConfig(
    level=logging.WARNING,  # Menos verbose para focar nos resultados
    format='%(levelname)s - %(message)s'
)

class PerformanceBenchmark:
    """Classe para executar benchmarks de performance"""
    
    def __init__(self):
        self.tester = CMShopMessageTester()
        self.results = []
        
    def run_throughput_test(self, message_counts: list, queue_name: str = Queues.CHECKOUT):
        """Testa throughput com diferentes quantidades de mensagens"""
        print("\nğŸš€ TESTE DE THROUGHPUT")
        print("=" * 50)
        
        results = []
        
        for count in message_counts:
            print(f"\nğŸ“Š Testando {count} mensagens...")
            
            if not self.tester.connect():
                continue
                
            if not self.tester.setup_queues():
                continue
            
            # Limpa a fila antes do teste
            try:
                self.tester.channel.queue_purge(queue_name)
            except:
                pass
            
            # Executa o teste
            sent, errors, duration, rate = self.tester.send_bulk_messages(queue_name, count, 0)
            
            result = {
                'test': 'throughput',
                'messages': count,
                'sent': sent,
                'errors': errors,
                'duration': duration,
                'rate': rate,
                'queue': queue_name,
                'timestamp': datetime.now().isoformat()
            }
            
            results.append(result)
            
            print(f"âœ… Resultado: {rate:.2f} msg/s")
            
            self.tester.close_connection()
            time.sleep(2)  # Pausa entre testes
        
        self.results.extend(results)
        return results
    
    def run_concurrency_test(self, message_count: int, thread_counts: list, queue_name: str = Queues.CHECKOUT):
        """Testa performance com diferentes nÃºmeros de threads"""
        print("\nğŸ”¥ TESTE DE CONCORRÃŠNCIA")
        print("=" * 50)
        
        results = []
        
        for threads in thread_counts:
            print(f"\nğŸ§µ Testando com {threads} threads...")
            
            if not self.tester.connect():
                continue
                
            if not self.tester.setup_queues():
                continue
            
            # Limpa a fila antes do teste
            try:
                self.tester.channel.queue_purge(queue_name)
            except:
                pass
            
            self.tester.close_connection()
            
            # Executa o teste concorrente
            sent, errors, duration, rate = self.tester.run_concurrent_load_test(
                queue_name, message_count, threads
            )
            
            result = {
                'test': 'concurrency',
                'messages': message_count,
                'threads': threads,
                'sent': sent,
                'errors': errors,
                'duration': duration,
                'rate': rate,
                'queue': queue_name,
                'timestamp': datetime.now().isoformat()
            }
            
            results.append(result)
            
            print(f"âœ… Resultado: {rate:.2f} msg/s com {threads} threads")
            
            time.sleep(3)  # Pausa entre testes
        
        self.results.extend(results)
        return results
    
    def run_stress_test(self, queue_name: str = Queues.CHECKOUT):
        """Executa teste de stress com alta carga"""
        print("\nğŸ’¥ TESTE DE STRESS")
        print("=" * 50)
        
        # ConfiguraÃ§Ãµes de stress
        stress_configs = [
            {'messages': 1000, 'threads': 10, 'name': 'Stress MÃ©dio'},
            {'messages': 5000, 'threads': 15, 'name': 'Stress Alto'},
            {'messages': 10000, 'threads': 20, 'name': 'Stress Extremo'}
        ]
        
        results = []
        
        for config in stress_configs:
            print(f"\nâš¡ {config['name']}: {config['messages']} mensagens, {config['threads']} threads")
            
            start_time = time.time()
            
            sent, errors, duration, rate = self.tester.run_concurrent_load_test(
                queue_name, config['messages'], config['threads']
            )
            
            # Verifica estado das filas apÃ³s o teste
            if self.tester.connect():
                try:
                    method = self.tester.channel.queue_declare(queue=queue_name, passive=True)
                    queue_size = method.method.message_count
                except:
                    queue_size = -1
                    
                self.tester.close_connection()
            else:
                queue_size = -1
            
            result = {
                'test': 'stress',
                'config': config['name'],
                'messages': config['messages'],
                'threads': config['threads'],
                'sent': sent,
                'errors': errors,
                'duration': duration,
                'rate': rate,
                'queue_size_after': queue_size,
                'queue': queue_name,
                'timestamp': datetime.now().isoformat()
            }
            
            results.append(result)
            
            print(f"âœ… {config['name']} concluÃ­do:")
            print(f"   â€¢ Taxa: {rate:.2f} msg/s")
            print(f"   â€¢ Mensagens na fila: {queue_size}")
            print(f"   â€¢ Errors: {errors}")
            
            time.sleep(5)  # Pausa maior entre testes de stress
        
        self.results.extend(results)
        return results
    
    def run_latency_test(self, message_count: int = 100):
        """Testa latÃªncia de envio individual de mensagens"""
        print("\nâ±ï¸  TESTE DE LATÃŠNCIA")
        print("=" * 50)
        
        if not self.tester.connect():
            return []
            
        if not self.tester.setup_queues():
            return []
        
        latencies = []
        
        print(f"ğŸ“Š Testando latÃªncia com {message_count} mensagens individuais...")
        
        for i in range(message_count):
            message = self.tester.create_sample_checkout_message(f"latency_test_{i}")
            
            start_time = time.time()
            success = self.tester.publish_message(Queues.CHECKOUT, message)
            end_time = time.time()
            
            if success:
                latency = (end_time - start_time) * 1000  # em milissegundos
                latencies.append(latency)
            
            if (i + 1) % 20 == 0:
                print(f"   Progresso: {i + 1}/{message_count}")
        
        if latencies:
            avg_latency = statistics.mean(latencies)
            min_latency = min(latencies)
            max_latency = max(latencies)
            p95_latency = statistics.quantiles(latencies, n=20)[18]  # 95Âº percentil
            
            result = {
                'test': 'latency',
                'messages': len(latencies),
                'avg_latency_ms': avg_latency,
                'min_latency_ms': min_latency,
                'max_latency_ms': max_latency,
                'p95_latency_ms': p95_latency,
                'timestamp': datetime.now().isoformat()
            }
            
            print(f"âœ… Resultados de LatÃªncia:")
            print(f"   â€¢ MÃ©dia: {avg_latency:.2f}ms")
            print(f"   â€¢ MÃ­nima: {min_latency:.2f}ms")
            print(f"   â€¢ MÃ¡xima: {max_latency:.2f}ms")
            print(f"   â€¢ 95Âº percentil: {p95_latency:.2f}ms")
            
            self.results.append(result)
            self.tester.close_connection()
            return [result]
        
        self.tester.close_connection()
        return []
    
    def generate_report(self):
        """Gera relatÃ³rio completo dos benchmarks"""
        if not self.results:
            print("âŒ Nenhum resultado de benchmark disponÃ­vel")
            return
        
        print("\n" + "=" * 80)
        print("ğŸ“ˆ RELATÃ“RIO COMPLETO DE PERFORMANCE")
        print("=" * 80)
        
        # Agrupa resultados por tipo de teste
        tests_by_type = {}
        for result in self.results:
            test_type = result['test']
            if test_type not in tests_by_type:
                tests_by_type[test_type] = []
            tests_by_type[test_type].append(result)
        
        # RelatÃ³rio por tipo de teste
        for test_type, test_results in tests_by_type.items():
            print(f"\nğŸ¯ {test_type.upper()}")
            print("-" * 40)
            
            if test_type == 'throughput':
                print("Mensagens    Enviadas    Taxa (msg/s)    DuraÃ§Ã£o")
                print("-" * 50)
                for result in test_results:
                    print(f"{result['messages']:>8}    {result['sent']:>8}    {result['rate']:>10.2f}    {result['duration']:>7.2f}s")
            
            elif test_type == 'concurrency':
                print("Threads    Mensagens    Enviadas    Taxa (msg/s)    DuraÃ§Ã£o")
                print("-" * 60)
                for result in test_results:
                    print(f"{result['threads']:>7}    {result['messages']:>9}    {result['sent']:>8}    {result['rate']:>10.2f}    {result['duration']:>7.2f}s")
            
            elif test_type == 'stress':
                print("ConfiguraÃ§Ã£o         Mensagens    Taxa (msg/s)    Erros    Fila Final")
                print("-" * 70)
                for result in test_results:
                    print(f"{result['config']:<18}    {result['messages']:>9}    {result['rate']:>10.2f}    {result['errors']:>5}    {result['queue_size_after']:>10}")
            
            elif test_type == 'latency':
                for result in test_results:
                    print(f"Mensagens testadas: {result['messages']}")
                    print(f"LatÃªncia mÃ©dia: {result['avg_latency_ms']:.2f}ms")
                    print(f"LatÃªncia mÃ­nima: {result['min_latency_ms']:.2f}ms")
                    print(f"LatÃªncia mÃ¡xima: {result['max_latency_ms']:.2f}ms")
                    print(f"95Âº percentil: {result['p95_latency_ms']:.2f}ms")
        
        # Salva relatÃ³rio em arquivo JSON
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"benchmark_report_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ RelatÃ³rio salvo em: {filename}")
        
        # Resumo final
        print(f"\nğŸ“Š RESUMO EXECUTIVO")
        print(f"-" * 30)
        
        # Melhor throughput
        throughput_results = [r for r in self.results if r['test'] == 'throughput']
        if throughput_results:
            best_throughput = max(throughput_results, key=lambda x: x['rate'])
            print(f"ğŸš€ Melhor throughput: {best_throughput['rate']:.2f} msg/s ({best_throughput['messages']} mensagens)")
        
        # Melhor concorrÃªncia
        concurrency_results = [r for r in self.results if r['test'] == 'concurrency']
        if concurrency_results:
            best_concurrency = max(concurrency_results, key=lambda x: x['rate'])
            print(f"ğŸ”¥ Melhor concorrÃªncia: {best_concurrency['rate']:.2f} msg/s ({best_concurrency['threads']} threads)")
        
        # LatÃªncia mÃ©dia
        latency_results = [r for r in self.results if r['test'] == 'latency']
        if latency_results:
            avg_latency = latency_results[0]['avg_latency_ms']
            print(f"â±ï¸  LatÃªncia mÃ©dia: {avg_latency:.2f}ms")

def main():
    """Executa suite completa de benchmarks"""
    print("\n" + "=" * 80)
    print("âš¡ BENCHMARK DE PERFORMANCE - CMSHOP MESSAGING")
    print("=" * 80)
    
    benchmark = PerformanceBenchmark()
    
    print("\nğŸ¯ SUITE DE TESTES DISPONÃVEIS:")
    print("1. ğŸš€ Teste de Throughput (diferentes quantidades)")
    print("2. ğŸ”¥ Teste de ConcorrÃªncia (diferentes threads)")
    print("3. ğŸ’¥ Teste de Stress (alta carga)")
    print("4. â±ï¸  Teste de LatÃªncia")
    print("5. ğŸª SUITE COMPLETA (todos os testes)")
    
    choice = input("\nEscolha uma opÃ§Ã£o (1-5): ").strip()
    
    start_time = time.time()
    
    try:
        if choice == '1':
            # Teste de throughput
            message_counts = [10, 50, 100, 500, 1000]
            benchmark.run_throughput_test(message_counts)
            
        elif choice == '2':
            # Teste de concorrÃªncia
            thread_counts = [1, 2, 5, 10, 15]
            benchmark.run_concurrency_test(500, thread_counts)
            
        elif choice == '3':
            # Teste de stress
            benchmark.run_stress_test()
            
        elif choice == '4':
            # Teste de latÃªncia
            benchmark.run_latency_test(100)
            
        elif choice == '5':
            # Suite completa
            print("\nğŸª Executando SUITE COMPLETA de benchmarks...")
            print("âš ï¸  Isto pode levar vÃ¡rios minutos...")
            
            confirm = input("Continuar? (s/N): ").strip().lower()
            if confirm != 's':
                print("Cancelado pelo usuÃ¡rio")
                return
            
            # Throughput
            print("\n" + "ğŸš€" * 20 + " THROUGHPUT " + "ğŸš€" * 20)
            benchmark.run_throughput_test([50, 100, 500, 1000])
            
            # ConcorrÃªncia
            print("\n" + "ğŸ”¥" * 20 + " CONCORRÃŠNCIA " + "ğŸ”¥" * 20)
            benchmark.run_concurrency_test(500, [1, 3, 5, 10])
            
            # LatÃªncia
            print("\n" + "â±ï¸ " * 20 + " LATÃŠNCIA " + "â±ï¸ " * 20)
            benchmark.run_latency_test(50)
            
            # Stress (versÃ£o reduzida para suite completa)
            print("\n" + "ğŸ’¥" * 20 + " STRESS " + "ğŸ’¥" * 20)
            stress_configs = [
                {'messages': 500, 'threads': 8, 'name': 'Stress MÃ©dio'},
                {'messages': 2000, 'threads': 12, 'name': 'Stress Alto'}
            ]
            
            for config in stress_configs:
                print(f"\nâš¡ {config['name']}: {config['messages']} mensagens, {config['threads']} threads")
                sent, errors, duration, rate = benchmark.tester.run_concurrent_load_test(
                    Queues.CHECKOUT, config['messages'], config['threads']
                )
                
                result = {
                    'test': 'stress',
                    'config': config['name'],
                    'messages': config['messages'],
                    'threads': config['threads'],
                    'sent': sent,
                    'errors': errors,
                    'duration': duration,
                    'rate': rate,
                    'queue': Queues.CHECKOUT,
                    'timestamp': datetime.now().isoformat()
                }
                
                benchmark.results.append(result)
                time.sleep(3)
        
        else:
            print("âŒ OpÃ§Ã£o invÃ¡lida")
            return
        
        # Gera relatÃ³rio final
        total_time = time.time() - start_time
        print(f"\nâ° Tempo total de execuÃ§Ã£o: {total_time:.2f} segundos")
        benchmark.generate_report()
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Benchmark interrompido pelo usuÃ¡rio")
        if benchmark.results:
            print("ğŸ“Š Gerando relatÃ³rio dos testes concluÃ­dos...")
            benchmark.generate_report()
    except Exception as e:
        print(f"\nâŒ Erro durante benchmark: {e}")

if __name__ == "__main__":
    main()